#!/usr/bin/env python3
"""
Test script for improved chunking system

This script demonstrates the difference between the old and new chunking approaches.
"""

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from ingestion.pipeline import _simple_chunk, _improved_chunk_document
from ingestion.improved_chunking import improved_chunk_text, ChunkConfig

def test_chunking_comparison():
    """Compare old vs new chunking approaches."""
    
    # Sample Korean legal text
    sample_text = """
    ì œ1ì¡° (ëª©ì ) ì´ ë²•ì€ ì „ìê¸ˆìœµê±°ë˜ì˜ ì•ˆì „ì„±ê³¼ ì‹ ë¢°ì„±ì„ í™•ë³´í•˜ê³  ì´ìš©ìë¥¼ ë³´í˜¸í•˜ê¸° ìœ„í•˜ì—¬ í•„ìš”í•œ ì‚¬í•­ì„ ê·œì •í•¨ì„ ëª©ì ìœ¼ë¡œ í•œë‹¤.
    
    ì œ2ì¡° (ì •ì˜) ì´ ë²•ì—ì„œ ì‚¬ìš©í•˜ëŠ” ìš©ì–´ì˜ ëœ»ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
    1. "ì „ìê¸ˆìœµê±°ë˜"ë€ ê¸ˆìœµê¸°ê´€ì´ ì „ìì  ì¥ì¹˜ë¥¼ í†µí•˜ì—¬ ê¸ˆìœµìƒí’ˆ ë° ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê³ , ì´ìš©ìê°€ ê¸ˆìœµê¸°ê´€ì˜ ì¢…ì‚¬ìì™€ ì§ì ‘ ëŒ€ë©´í•˜ê±°ë‚˜ ì˜ì‚¬ì†Œí†µì„ í•˜ì§€ ì•„ë‹ˆí•˜ê³  ìë™í™”ëœ ë°©ì‹ìœ¼ë¡œ ì´ë¥¼ ì´ìš©í•˜ëŠ” ê±°ë˜ë¥¼ ë§í•œë‹¤.
    2. "ì „ìì  ì¥ì¹˜"ë€ ì „ìê¸ˆìœµì—…ë¬´ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•˜ì—¬ ì´ìš©ë˜ëŠ” ì „ìì  ë°©ë²•ìœ¼ë¡œì„œ ëŒ€í†µë ¹ë ¹ìœ¼ë¡œ ì •í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤.
    3. "ê¸ˆìœµê¸°ê´€"ì´ë€ ë‹¤ìŒ ê° í˜¸ì˜ ì–´ëŠ í•˜ë‚˜ì— í•´ë‹¹í•˜ëŠ” ê¸°ê´€ì„ ë§í•œë‹¤.
    ê°€. ì€í–‰ë²•ì— ë”°ë¥¸ ì€í–‰
    ë‚˜. ìë³¸ì‹œì¥ê³¼ ê¸ˆìœµíˆ¬ìì—…ì— ê´€í•œ ë²•ë¥ ì— ë”°ë¥¸ íˆ¬ìë§¤ë§¤ì—…ì, íˆ¬ìì¤‘ê°œì—…ì, ì§‘í•©íˆ¬ìì—…ì, ì‹ íƒì—…ì, ì¦ê¶Œê¸ˆìœµíšŒì‚¬, ì¢…í•©ê¸ˆìœµíšŒì‚¬ ë° ëª…ì˜ê°œì„œëŒ€í–‰íšŒì‚¬
    ë‹¤. ë³´í—˜ì—…ë²•ì— ë”°ë¥¸ ë³´í—˜íšŒì‚¬
    ë¼. ìƒí˜¸ì €ì¶•ì€í–‰ë²•ì— ë”°ë¥¸ ìƒí˜¸ì €ì¶•ì€í–‰
    ë§ˆ. ì‹ ìš©í˜‘ë™ì¡°í•©ë²•ì— ë”°ë¥¸ ì‹ ìš©í˜‘ë™ì¡°í•©
    ë°”. ìƒˆë§ˆì„ê¸ˆê³ ë²•ì— ë”°ë¥¸ ìƒˆë§ˆì„ê¸ˆê³ 
    ì‚¬. ì‚°ë¦¼ì¡°í•©ë²•ì— ë”°ë¥¸ ì‚°ë¦¼ì¡°í•©
    ì•„. ë†í˜‘ë²•ì— ë”°ë¥¸ ë†í˜‘ì¤‘ì•™íšŒ ë° ì§€ì—­ë†í˜‘
    ì. ìˆ˜í˜‘ë²•ì— ë”°ë¥¸ ìˆ˜í˜‘ì¤‘ì•™íšŒ ë° ì§€ì—­ìˆ˜í˜‘
    ì°¨. ê¸°íƒ€ ëŒ€í†µë ¹ë ¹ìœ¼ë¡œ ì •í•˜ëŠ” ê¸ˆìœµê¸°ê´€
    
    ì œ3ì¡° (ì ìš©ë²”ìœ„) ì´ ë²•ì€ ì „ìê¸ˆìœµê±°ë˜ì— ëŒ€í•˜ì—¬ ì ìš©í•œë‹¤. ë‹¤ë§Œ, ë‹¤ìŒ ê° í˜¸ì˜ ì–´ëŠ í•˜ë‚˜ì— í•´ë‹¹í•˜ëŠ” ê²½ìš°ì—ëŠ” ê·¸ëŸ¬í•˜ì§€ ì•„ë‹ˆí•˜ë‹¤.
    1. ì „ìê¸ˆìœµê±°ë˜ì˜ ë‹¹ì‚¬ì ê°„ì— ë³„ë„ì˜ ì•½ì •ì´ ìˆëŠ” ê²½ìš°
    2. ì „ìê¸ˆìœµê±°ë˜ì˜ íŠ¹ì„±ìƒ ì´ ë²•ì˜ ì ìš©ì´ ë¶€ì ì ˆí•˜ë‹¤ê³  ëŒ€í†µë ¹ë ¹ìœ¼ë¡œ ì •í•˜ëŠ” ê²½ìš°
    """
    
    print("=" * 80)
    print("CHUNKING COMPARISON TEST")
    print("=" * 80)
    
    # Test old chunking
    print("\nğŸ”´ OLD CHUNKING (Character-based):")
    print("-" * 50)
    old_chunks = _simple_chunk(sample_text, max_chars=200, overlap=50)
    for i, chunk in enumerate(old_chunks):
        print(f"Chunk {i+1} ({len(chunk)} chars):")
        print(f"  {chunk[:100]}...")
        print()
    
    # Test new chunking
    print("\nğŸŸ¢ NEW CHUNKING (Token-based, Semantic-aware):")
    print("-" * 50)
    
    doc_metadata = {
        "doc_id": "test_doc",
        "section": "test_section",
        "lang": "ko"
    }
    
    new_chunks = _improved_chunk_document(sample_text, doc_metadata)
    for i, chunk_data in enumerate(new_chunks):
        print(f"Chunk {i+1}:")
        print(f"  Tokens: {chunk_data.get('token_count', 'N/A')}")
        print(f"  Characters: {chunk_data.get('char_count', 'N/A')}")
        print(f"  Entities: {chunk_data.get('entities', [])}")
        print(f"  Text: {chunk_data['body'][:100]}...")
        print()
    
    # Test direct improved chunking
    print("\nğŸ”µ DIRECT IMPROVED CHUNKING:")
    print("-" * 50)
    
    config = ChunkConfig(
        max_tokens=100,
        min_tokens=20,
        overlap_tokens=20,
        respect_sentences=True,
        respect_paragraphs=True
    )
    
    direct_chunks = improved_chunk_text(sample_text, config)
    for i, chunk_data in enumerate(direct_chunks):
        print(f"Chunk {i+1}:")
        print(f"  Tokens: {chunk_data.get('token_count', 'N/A')}")
        print(f"  Entities: {chunk_data.get('entities', [])}")
        print(f"  Text: {chunk_data['body'][:100]}...")
        print()

if __name__ == "__main__":
    test_chunking_comparison()
