"""
Adaptive Graph - Executes dynamic workflows generated by Meta Agent
"""
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime

from langchain_core.language_models import BaseLanguageModel
from agent.meta_agent import MetaAgent
from agent.python_runtime import PythonRuntime
from agent.types import Answer
from configs.load import setup_root_logger, get_default_llm
from agent.nodes.observer import notify_observers

# Ensure logger exists
_logger = setup_root_logger()


def run_adaptive_graph(query: str, time_hint: str | None, lang: str | None, trace_id: str, 
                      session_id: str | None = None, llm: Optional[BaseLanguageModel] = None) -> Dict[str, Any]:
    """
    Run an adaptive graph that dynamically generates workflows based on query complexity.
    """
    
    # Add trace filter
    class _TraceFilter(logging.Filter):
        def filter(self, record: logging.LogRecord) -> bool:
            setattr(record, "trace_id", trace_id)
            return True

    tf = _TraceFilter()
    _logger.addFilter(tf)
    
    try:
        _logger.info("adaptive_graph_start", extra={
            "trace_id": trace_id, 
            "query": query, 
            "time_hint": time_hint, 
            "lang": lang, 
            "session_id": session_id
        })

        # Use session_id from trace_id if not provided
        if not session_id:
            session_id = trace_id
        
        # Initialize meta agent and python runtime
        meta_agent = MetaAgent(llm=llm)
        python_runtime = PythonRuntime()
        
        # Step 1: Add user message to conversation memory
        from memory.conversation_memory import conversation_memory
        
        # Add user message to conversation memory
        conversation_memory.add_user_message(session_id, query)
        
        # Step 2: Analyze conversation context FIRST using Context Agent
        notify_observers("context_agent", "in_progress", {
            "query": query,
            "session_id": session_id
        })
        
        from agent.nodes.context_agent import analyze_conversation_context
        
        # Get conversation history for context analysis
        history = conversation_memory._conversations.get(session_id, {}).get("history", [])
        context_analysis = analyze_conversation_context(session_id, query, history)
        
        notify_observers("context_agent", "completed", {
            "is_follow_up": context_analysis.get("is_follow_up", False),
            "conversation_topic": context_analysis.get("conversation_topic"),
            "context_relevance": context_analysis.get("context_relevance", "low"),
            "suggested_workflow_type": context_analysis.get("suggested_workflow_type", "simple_search")
        })
        
        # Step 3: Meta Agent uses context analysis to make informed workflow decisions
        notify_observers("meta_agent", "in_progress", {
            "query": query,
            "context_analysis": context_analysis
        })
        
        # Create conversation context for Meta Agent
        conversation_context = {
            "context_analysis": context_analysis,
            "conversation_history": history
        }
        
        # Analyze query complexity and generate workflow with context analysis
        workflow_config = meta_agent.analyze_query_complexity(query, conversation_context)
        
        # Generate workflow schema if not provided
        if "workflow_schema" not in workflow_config:
            workflow_config["workflow_schema"] = meta_agent.generate_workflow_schema(workflow_config)
        
        _logger.info("meta_agent_complete", extra={
            "trace_id": trace_id, 
            "workflow_type": workflow_config.get("workflow_type"),
            "complexity_score": workflow_config.get("complexity_score")
        })
        notify_observers("meta_agent", "completed", {
            "workflow_type": workflow_config.get("workflow_type"),
            "complexity_score": workflow_config.get("complexity_score"),
            "reasoning": workflow_config.get("reasoning"),
            "workflow_schema": workflow_config.get("workflow_schema"),
            "agent_summary": workflow_config.get("agent_summary"),
            "context_awareness": workflow_config.get("context_awareness", {}),
            "context_influence": workflow_config.get("context_influence", "")
        })
        
        # Step 2: Execute workflow based on type
        workflow_type = workflow_config.get("workflow_type", "simple_search")
        
        if workflow_type == "simple_search":
            return _execute_simple_search(query, trace_id, session_id, llm)
        
        elif workflow_type == "complex_filtering":
            return _execute_complex_filtering(query, workflow_config, trace_id, session_id, llm, python_runtime)
        
        elif workflow_type == "computation_required":
            return _execute_computation_workflow(query, workflow_config, trace_id, session_id, llm, python_runtime)
        
        elif workflow_type == "monitoring_workflow":
            return _execute_monitoring_workflow(query, workflow_config, trace_id, session_id, llm)
        
        else:
            # No fallback - let it fail clearly
            raise Exception(f"Unknown workflow type: {workflow_type}")
    
    except Exception as e:
        _logger.exception(f"Error in adaptive graph: {e}")
        notify_observers("error", "failed", {"error": str(e)})
        return {
            "text": f"I encountered an error processing your request: {str(e)}",
            "citations": [],
            "session_id": session_id,
            "has_context": False,
            "trace_id": trace_id
        }
    
    finally:
        _logger.removeFilter(tf)


def _execute_simple_search(query: str, trace_id: str, session_id: str, llm) -> Dict[str, Any]:
    """Execute a simple search workflow without the old graph nodes."""
    
    # Step 1: Semantic search
    notify_observers("semantic_search", "in_progress", {"query": query})
    
    from agent.nodes.candidate_search_chroma import first_pass_search
    search_results = first_pass_search(query=query, alpha=0.5)
    
    _logger.info(f"Simple search: Retrieved {len(search_results)} candidates")
    notify_observers("semantic_search", "completed", {"count": len(search_results)})
    
    # Step 2: Generate answer
    notify_observers("answerer", "in_progress", {"results_count": len(search_results)})
    
    from agent.nodes.answerer import compose_answer
    answer = compose_answer(query=query, top=search_results[:10], llm=llm)  # Use top 10 results
    
    # Store in conversation memory
    from memory.conversation_memory import conversation_memory
    conversation_memory.add_assistant_message(
        session_id=session_id,
        message=answer.get("text", ""),
        citations=answer.get("citations", [])
    )
    
    _logger.info("simple_search_complete", extra={
        "trace_id": trace_id, 
        "results_count": len(search_results),
        "answer_length": len(answer.get("text", ""))
    })
    notify_observers("answerer", "completed", {
        "text_length": len(answer.get("text", "")),
        "citations_count": len(answer.get("citations", []))
    })
    
    return {
        "text": answer.get("text", ""), 
        "citations": answer.get("citations", []),
        "session_id": session_id,
        "has_context": True,
        "trace_id": trace_id
    }


def _execute_complex_filtering(query: str, workflow_config: Dict[str, Any], trace_id: str, 
                              session_id: str, llm, python_runtime: PythonRuntime) -> Dict[str, Any]:
    """Execute complex filtering workflow."""
    
    # Step 1: Get large pool of results
    notify_observers("semantic_search", "in_progress", {"query": query})
    
    from agent.nodes.candidate_search_chroma import first_pass_search
    from adapters.chroma_adapter import ChromaClient
    
    with ChromaClient() as client:
        if not client._connected:
            return {
                "text": "Database connection error. Please try again.",
                "citations": [],
                "session_id": session_id,
                "has_context": False,
                "trace_id": trace_id
            }
        
        # Get large pool of results
        large_pool = first_pass_search(query=query, alpha=0.5)
        _logger.info(f"Complex filtering: Retrieved {len(large_pool)} candidates")
        notify_observers("semantic_search", "completed", {"count": len(large_pool)})
    
    # Step 2: Apply complex filtering with Python runtime
    notify_observers("complex_filtering", "in_progress", {"candidates_count": len(large_pool)})
    
    # Use soft filtering system for complex queries
    from soft_boost_filtering import SoftBoostFilter
    soft_filter = SoftBoostFilter()
    
    # Apply soft boosting to rank results by relevance
    boost_info = soft_filter.apply_soft_boosting(large_pool, query)
    boosted_results = boost_info['boosted_chunks']
    
    # Take top results (limit to reasonable number)
    filtered_results = [chunk for chunk, score in boosted_results[:50]]  # Top 50 results
    
    _logger.info(f"Complex filtering: Soft filtered to {len(filtered_results)} results")
    notify_observers("complex_filtering", "completed", {
        "filtered_count": len(filtered_results),
        "original_count": len(large_pool),
        "method": "soft_filtering",
        "top_scores": [f"{score:.2f}" for chunk, score in boosted_results[:3]],
        "boost_info": {
            "winners": boost_info['winners'],
            "losers": boost_info['losers'],
            "query_intent": boost_info['query_intent'],
            "total_boosted": len(boost_info['boost_details'])
        }
    })
    
    # Step 3: Generate answer
    notify_observers("answerer", "in_progress", {"results_count": len(filtered_results)})
    
    from agent.nodes.answerer import compose_answer
    answer = compose_answer(query=query, top=filtered_results, llm=llm)
    
    # Store in conversation memory
    from memory.conversation_memory import conversation_memory
    conversation_memory.add_assistant_message(
        session_id=session_id,
        message=answer.get("text", ""),
        citations=answer.get("citations", [])
    )
    
    notify_observers("answerer", "completed", {
        "text": answer.get("text", ""),
        "citations_count": len(answer.get("citations", []))
    })
    
    return {
        "text": answer.get("text", ""),
        "citations": answer.get("citations", []),
        "session_id": session_id,
        "has_context": True,
        "trace_id": trace_id,
        "workflow_type": "complex_filtering"
    }


def _execute_computation_workflow(query: str, workflow_config: Dict[str, Any], trace_id: str, 
                                 session_id: str, llm, python_runtime: PythonRuntime) -> Dict[str, Any]:
    """Execute computation-heavy workflow (like Tuesday meeting analysis)."""
    
    # Step 1: Get comprehensive search results
    notify_observers("comprehensive_search", "in_progress", {"query": query})
    
    from agent.nodes.candidate_search_chroma import first_pass_search
    from adapters.chroma_adapter import ChromaClient
    
    with ChromaClient() as client:
        if not client._connected:
            return {
                "text": "Database connection error. Please try again.",
                "citations": [],
                "session_id": session_id,
                "has_context": False,
                "trace_id": trace_id
            }
        
        # Get large pool with high alpha for comprehensive results
        comprehensive_results = first_pass_search(query=query, alpha=0.8)
        _logger.info(f"Computation workflow: Retrieved {len(comprehensive_results)} candidates")
        notify_observers("comprehensive_search", "completed", {"count": len(comprehensive_results)})
    
    # Step 2: Extract and analyze data
    notify_observers("data_extraction", "in_progress", {"results_count": len(comprehensive_results)})
    
    # Extract all dates and metadata for analysis
    extracted_data = []
    for result in comprehensive_results:
        metadata = result.get('metadata', {})
        extracted_data.append({
            'doc_id': result.get('doc_id', ''),
            'meeting_date': metadata.get('meeting_date', ''),
            'attendees': metadata.get('attendees', ''),
            'body': result.get('body', ''),
            'metadata': metadata
        })
    
    _logger.info(f"Data extraction: Processed {len(extracted_data)} documents")
    notify_observers("data_extraction", "completed", {"extracted_count": len(extracted_data)})
    
    # Step 3: Apply computation logic
    notify_observers("computation", "in_progress", {"data_count": len(extracted_data)})
    
    python_code = workflow_config.get("python_code", "")
    if python_code:
        computed_results = python_runtime.execute_code(python_code, extracted_data)
        _logger.info(f"Computation: Processed {len(extracted_data)} -> {len(computed_results)} results")
        notify_observers("computation", "completed", {
            "input_count": len(extracted_data),
            "output_count": len(computed_results),
            "computation_type": "custom_python"
        })
    else:
        # Fallback computation
        computed_results = _fallback_computation(query, extracted_data)
        notify_observers("computation", "completed", {
            "input_count": len(extracted_data),
            "output_count": len(computed_results),
            "computation_type": "fallback"
        })
    
    # Step 4: Generate comprehensive answer
    notify_observers("answerer", "in_progress", {"results_count": len(computed_results)})
    
    from agent.nodes.answerer import compose_answer
    answer = compose_answer(query=query, top=computed_results, llm=llm)
    
    # Store in conversation memory
    from memory.conversation_memory import conversation_memory
    conversation_memory.add_assistant_message(
        session_id=session_id,
        message=answer.get("text", ""),
        citations=answer.get("citations", [])
    )
    
    notify_observers("answerer", "completed", {
        "text": answer.get("text", ""),
        "citations_count": len(answer.get("citations", []))
    })
    
    return {
        "text": answer.get("text", ""),
        "citations": answer.get("citations", []),
        "session_id": session_id,
        "has_context": True,
        "trace_id": trace_id,
        "workflow_type": "computation_required",
        "computation_stats": {
            "total_documents": len(comprehensive_results),
            "filtered_results": len(computed_results)
        }
    }


def _fallback_computation(query: str, data: List[Dict]) -> List[Dict]:
    """Fallback computation when Python code generation fails."""
    # Simple keyword-based filtering as fallback
    query_lower = query.lower()
    
    if "tuesday" in query_lower or "화요일" in query_lower:
        # Try to filter for Tuesday meetings
        tuesday_results = []
        for item in data:
            meeting_date = item.get('meeting_date', '')
            if meeting_date:
                try:
                    from datetime import datetime
                    date_obj = datetime.fromisoformat(meeting_date.replace('Z', '+00:00'))
                    if date_obj.weekday() == 1:  # Tuesday is weekday 1
                        tuesday_results.append(item)
                except:
                    continue
        return tuesday_results
    
    return data


def _execute_monitoring_workflow(query: str, workflow_config: Dict[str, Any], trace_id: str, 
                                session_id: str, llm) -> Dict[str, Any]:
    """Execute a monitoring workflow with quality assessment and adaptive responses."""
    
    # Step 1: Perform initial search
    notify_observers("semantic_search", "in_progress", {"query": query})
    
    from agent.nodes.candidate_search_chroma import first_pass_search
    search_results = first_pass_search(query=query, alpha=0.5)
    
    _logger.info(f"Monitoring workflow: Retrieved {len(search_results)} candidates")
    notify_observers("semantic_search", "completed", {"count": len(search_results)})
    
    # Step 2: Assess search quality
    notify_observers("quality_monitor", "in_progress", {"assessment_type": "search_quality"})
    
    from agent.nodes.quality_monitor import monitor_search_quality
    search_quality = monitor_search_quality(query, search_results)
    
    _logger.info(f"Search quality assessment: {search_quality['quality_score']:.2f}")
    notify_observers("quality_monitor", "completed", {
        "assessment_type": "search_quality",
        "quality_score": search_quality['quality_score'],
        "issues": search_quality['issues'],
        "recommendations": search_quality['recommendations']
    })
    
    # Step 3: Adaptive response based on quality assessment
    if search_quality['quality_score'] < 0.6:
        notify_observers("adaptive_response", "in_progress", {
            "trigger": "low_search_quality",
            "quality_score": search_quality['quality_score']
        })
        
        # Try alternative search strategies
        _logger.info("Low search quality detected, trying alternative search strategies")
        
        # Try with different alpha values
        alternative_results = []
        for alpha in [0.3, 0.7]:
            alt_results = first_pass_search(query=query, alpha=alpha)
            alternative_results.extend(alt_results)
        
        # Use the best results
        if alternative_results:
            search_results = alternative_results[:len(search_results)]
            _logger.info(f"Alternative search yielded {len(search_results)} results")
        
        notify_observers("adaptive_response", "completed", {
            "action": "alternative_search",
            "new_result_count": len(search_results)
        })
    
    # Step 4: Generate answer
    notify_observers("answerer", "in_progress", {"results_count": len(search_results)})
    
    from agent.nodes.answerer import compose_answer
    answer = compose_answer(query=query, top=search_results[:10], llm=llm)
    
    # Step 5: Assess answer quality
    notify_observers("quality_monitor", "in_progress", {"assessment_type": "answer_quality"})
    
    from agent.nodes.quality_monitor import monitor_answer_quality
    answer_quality = monitor_answer_quality(query, answer, search_results)
    
    _logger.info(f"Answer quality assessment: {answer_quality['quality_score']:.2f}")
    notify_observers("quality_monitor", "completed", {
        "assessment_type": "answer_quality",
        "quality_score": answer_quality['quality_score'],
        "completeness_score": answer_quality['completeness_score'],
        "accuracy_score": answer_quality['accuracy_score'],
        "citation_score": answer_quality['citation_score'],
        "coherence_score": answer_quality['coherence_score'],
        "issues": answer_quality['issues'],
        "recommendations": answer_quality['recommendations']
    })
    
    # Step 6: Final adaptive response if answer quality is poor
    if answer_quality['quality_score'] < 0.6:
        notify_observers("adaptive_response", "in_progress", {
            "trigger": "low_answer_quality",
            "quality_score": answer_quality['quality_score']
        })
        
        _logger.info("Low answer quality detected, attempting to improve answer")
        
        # Try to improve the answer with more context
        improved_answer = compose_answer(
            query=query, 
            top=search_results[:20],  # Use more results
            llm=llm
        )
        
        # Re-assess improved answer
        improved_quality = monitor_answer_quality(query, improved_answer, search_results)
        
        if improved_quality['quality_score'] > answer_quality['quality_score']:
            answer = improved_answer
            answer_quality = improved_quality
            _logger.info(f"Answer improved: {answer_quality['quality_score']:.2f}")
        
        notify_observers("adaptive_response", "completed", {
            "action": "answer_improvement",
            "final_quality_score": answer_quality['quality_score']
        })
    
    # Store in conversation memory
    from memory.conversation_memory import conversation_memory
    conversation_memory.add_assistant_message(
        session_id=session_id,
        message=answer.get("text", ""),
        citations=answer.get("citations", [])
    )
    
    _logger.info("monitoring_workflow_complete", extra={
        "trace_id": trace_id, 
        "search_quality": search_quality['quality_score'],
        "answer_quality": answer_quality['quality_score']
    })
    
    notify_observers("answerer", "completed", {
        "text_length": len(answer.get("text", "")),
        "citations_count": len(answer.get("citations", [])),
        "final_quality_score": answer_quality['quality_score']
    })
    
    return {
        "text": answer.get("text", ""), 
        "citations": answer.get("citations", []),
        "session_id": session_id,
        "has_context": True,
        "trace_id": trace_id,
        "workflow_type": "monitoring_workflow",
        "quality_metrics": {
            "search_quality": search_quality,
            "answer_quality": answer_quality
        }
    }
